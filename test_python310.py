#!/usr/bin/env python3
"""
Python 3.10 Compatibility Test for REALM-Bench

This script tests if all dependencies work correctly with Python 3.10.
"""

import sys
import importlib
from typing import List, Dict, Any

def test_python_version():
    """Test Python version compatibility"""
    print("üêç Python Version Test")
    print("=" * 40)
    
    version = sys.version_info
    print(f"Python version: {version.major}.{version.minor}.{version.micro}")
    
    if version.major == 3 and version.minor >= 10:
        print("‚úÖ Python 3.10+ detected - Compatible!")
    else:
        print("‚ùå Python 3.10+ required!")
        return False
    
    print()
    return True

def test_core_dependencies():
    """Test core dependencies"""
    print("üì¶ Core Dependencies Test")
    print("=" * 40)
    
    dependencies = [
        "pandas",
        "numpy", 
        "matplotlib",
        "seaborn",
        "psutil",
        "json",
        "time",
        "os",
        "sys"
    ]
    
    failed = []
    for dep in dependencies:
        try:
            importlib.import_module(dep)
            print(f"‚úÖ {dep}")
        except ImportError as e:
            print(f"‚ùå {dep}: {e}")
            failed.append(dep)
    
    if failed:
        print(f"\n‚ùå Failed to import: {failed}")
        return False
    else:
        print("\n‚úÖ All core dependencies available!")
    
    print()
    return True

def test_evaluation_framework():
    """Test evaluation framework imports"""
    print("üîß Evaluation Framework Test")
    print("=" * 40)
    
    try:
        from evaluation.task_definitions import TASK_DEFINITIONS
        print("‚úÖ Task definitions imported")
        
        from evaluation.metrics import PlanningQualityMetrics
        print("‚úÖ Metrics classes imported")
        
        from evaluation.evaluator import BenchmarkEvaluator, EvaluationConfig
        print("‚úÖ Evaluator classes imported")
        
        from evaluation.framework_runners import get_framework_runners
        print("‚úÖ Framework runners imported")
        
        print(f"‚úÖ Available tasks: {list(TASK_DEFINITIONS.keys())}")
        
    except ImportError as e:
        print(f"‚ùå Evaluation framework import failed: {e}")
        return False
    
    print()
    return True

def test_basic_functionality():
    """Test basic functionality"""
    print("‚öôÔ∏è Basic Functionality Test")
    print("=" * 40)
    
    try:
        # Test task definition
        from evaluation.task_definitions import TASK_DEFINITIONS
        task = TASK_DEFINITIONS["P11"]
        print(f"‚úÖ Task P11 loaded: {task.name}")
        
        # Test metrics
        from evaluation.metrics import PlanningQualityMetrics
        metrics = PlanningQualityMetrics()
        print("‚úÖ Metrics instance created")
        
        # Test evaluation config
        from evaluation.evaluator import EvaluationConfig
        config = EvaluationConfig(
            frameworks=["langgraph"],
            tasks=["P11"],
            num_runs=1,
            timeout_seconds=60
        )
        print("‚úÖ Evaluation config created")
        
        # Test mock runner
        from evaluation.framework_runners import create_mock_runner
        mock_runner = create_mock_runner("test")
        print("‚úÖ Mock runner created")
        
    except Exception as e:
        print(f"‚ùå Basic functionality test failed: {e}")
        return False
    
    print("‚úÖ All basic functionality tests passed!")
    print()
    return True

def test_optional_frameworks():
    """Test optional framework availability"""
    print("üîå Optional Frameworks Test")
    print("=" * 40)
    
    frameworks = ["langgraph", "autogen", "crewai", "swarm"]
    
    for framework in frameworks:
        try:
            if framework == "langgraph":
                import langgraph
                print(f"‚úÖ {framework}: Available")
            elif framework == "autogen":
                import autogen
                print(f"‚úÖ {framework}: Available")
            elif framework == "crewai":
                import crewai
                print(f"‚úÖ {framework}: Available")
            elif framework == "swarm":
                # Swarm is installed via git, so we'll just check if it's importable
                print(f"‚úÖ {framework}: Available (git install)")
        except ImportError:
            print(f"‚ö†Ô∏è {framework}: Not available (will use mock runner)")
    
    print()

def main():
    """Run all tests"""
    print("üß™ REALM-Bench Python 3.10 Compatibility Test")
    print("=" * 60)
    print()
    
    tests = [
        ("Python Version", test_python_version),
        ("Core Dependencies", test_core_dependencies),
        ("Evaluation Framework", test_evaluation_framework),
        ("Basic Functionality", test_basic_functionality),
        ("Optional Frameworks", test_optional_frameworks)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå {test_name} test crashed: {e}")
            results.append((test_name, False))
    
    # Summary
    print("üìä Test Summary")
    print("=" * 40)
    
    passed = 0
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status}: {test_name}")
        if result:
            passed += 1
    
    print(f"\nResults: {passed}/{len(results)} tests passed")
    
    if passed == len(results):
        print("\nüéâ All tests passed! REALM-Bench is ready to use with Python 3.10!")
        print("\nYou can now run:")
        print("  python run_evaluation.py --mock")
    else:
        print("\n‚ö†Ô∏è Some tests failed. Please check the errors above.")
        print("You may need to install missing dependencies or fix configuration.")

if __name__ == "__main__":
    main() 